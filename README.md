# GitHub Data Scraper + Analyzer

A smart data analytics project that collects GitHub user and repository data using the GitHub REST API, stores it in a structured database, and provides analytical insights like most active developers, language usage, repo activity trends, and more.

---

## 🚀 Project Overview

This project aims to simulate a real-world data pipeline and analytics dashboard by integrating API data collection, database management, and data visualization.

### 🎯 Key Objectives
- Understand **API integration and data ingestion** workflows.
- Build **ETL (Extract, Transform, Load)** pipelines for large-scale data.
- Practice **SQL + ORM (SQLAlchemy)** for data management.
- Develop **visual analytics** to generate insights from real-world GitHub data.

---

## 🧩 Features

- **Data Scraper:** Fetches user and repository details from GitHub API.
- **Database Layer:** Stores data efficiently using SQLite or PostgreSQL.
- **Data Analyzer:** Performs metrics analysis (stars, forks, commits, etc.).
- **Visualization Dashboard:** Interactive charts for insights.
- **CLI Utility:** Run scrapers and analyzers from the command line.
- **REST API (Optional):** Expose analyzed data via Flask API.

---

## 🏗️ Tech Stack

| Layer | Technologies |
|--------|---------------|
| **Backend / API** | Flask, Flask-RESTful, SQLAlchemy |
| **Database** | SQLite (dev), PostgreSQL (prod) |
| **Data Analysis** | Pandas, NumPy, Matplotlib, Plotly |
| **Web Frontend (optional)** | Vue.js, Chart.js |
| **APIs Used** | GitHub REST API / GraphQL API |
| **CLI Tools** | Python `argparse` or `click` |

---

## 📁 Project Structure

```
github-data-analyzer/
│
├── backend/
│   ├── app.py
│   ├── models.py
│   ├── scraper/
│   │   ├── fetch_repos.py
│   │   ├── fetch_users.py
│   │   └── __init__.py
│   ├── analyzer/
│   │   ├── insights.py
│   │   └── stats.py
│   └── api/
│       ├── routes.py
│       └── __init__.py
│
├── cli/
│   ├── main.py
│   └── utils.py
│
├── frontend/ (optional)
│   └── src/
│
├── tests/
│   └── test_scraper.py
│
├── requirements.txt
├── README.md
└── .env.example
```

---

## 🧠 Learning Outcomes

By completing this project, you’ll gain experience with:
- REST APIs and authentication (GitHub OAuth tokens)
- Data collection and cleaning using Python
- ORM and SQL database design
- Data visualization for insights
- CLI + Web interface integration
- Scalable project architecture

---

## 🚦 Setup Instructions

### 1️⃣ Clone the repository
```bash
git clone https://github.com/23f2005532/GitHub-Data-Scraper-and-Analyzer.git
cd github-data-analyzer
```

### 2️⃣ Create a virtual environment
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

### 3️⃣ Install dependencies
```bash
pip install -r requirements.txt
```

### 4️⃣ Setup environment variables
Rename `.env.example` → `.env` and add your GitHub access token.

### 5️⃣ Run the scraper
```bash
python cli/main.py fetch --user <username>
```

### 6️⃣ Start the Flask API server
```bash
cd backend
python app.py
```

---

## 🌐 Optional Frontend Setup (Vue.js)
If you want to visualize the analytics:
```bash
cd frontend
npm install
npm run dev
```

---

## 📊 Example Insights
- Most popular programming languages across GitHub.
- Repository star/fork growth trends.
- Top active users by commits.
- Visual correlations between repo activity and popularity.

---

## 🎓 Author
**Ehtesham Ansari**  
IIT Madras BS in Data Science and Applications  


---

## 🧾 License
MIT License © 2025 Ehtesham Ansari
